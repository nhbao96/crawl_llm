{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0bd016-441e-43b9-a951-12b9374959d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\brian.nguyen\\appdata\\roaming\\python\\python312\\site-packages (4.24.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in d:\\apps\\anaconda\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\brian.nguyen\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\brian.nguyen\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in d:\\apps\\anaconda\\lib\\site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in d:\\apps\\anaconda\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in d:\\apps\\anaconda\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\brian.nguyen\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\apps\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\apps\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\brian.nguyen\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\apps\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\apps\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\brian.nguyen\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\apps\\anaconda\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\apps\\anaconda\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\apps\\anaconda\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\apps\\anaconda\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\apps\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in d:\\apps\\anaconda\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\apps\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\apps\\anaconda\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\apps\\anaconda\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\apps\\anaconda\\lib\\site-packages (from requests) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "def init_driver(chrome_driver_path):\n",
    "    service = Service(executable_path=chrome_driver_path)\n",
    "    return webdriver.Chrome(service=service)\n",
    "\n",
    "def scroll_to_end(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  \n",
    "        last_height = new_height\n",
    "\n",
    "def crawl(url, driver, output_file, depth=0, max_depth=3, visited_links=set(), content_hashes=set()):\n",
    "    # check max depth \n",
    "    if depth > max_depth:\n",
    "        return \n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {url}: {e}\")\n",
    "        return\n",
    "    \n",
    "    time.sleep(2)\n",
    "    scroll_to_end(driver)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # rm unnesserary tags\n",
    "    for tag in soup(['img', 'video']):\n",
    "        tag.decompose()\n",
    "    \n",
    "    # get content -> create hash to check overlap\n",
    "    content = ' '.join(soup.get_text(separator=\" \", strip=True).split())\n",
    "    content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    if content_hash in content_hashes:\n",
    "        return\n",
    "\n",
    "    visited_links.add(url)\n",
    "    content_hashes.add(content_hash)\n",
    "    \n",
    "    # write result to output file\n",
    "    with open(output_file, 'a', encoding='utf-8', newline='') as f:\n",
    "        f.write(f'{url} #{content}\\n\\n')\n",
    "\n",
    "    # find , handle child urls\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        new_url = link['href']\n",
    "        \n",
    "        # rm rss\n",
    "        if 'rss' in new_url:\n",
    "            continue \n",
    "\n",
    "        # change absolute\n",
    "        if new_url.startswith('/'):\n",
    "            new_url = url.rstrip('/') + new_url\n",
    "\n",
    "        # rm redundant in url\n",
    "        if \".htm\" in new_url and new_url.count(\".htm\") > 1:\n",
    "            new_url = new_url.replace(\".htm\", \"\", new_url.count(\".htm\") - 1)\n",
    "\n",
    "        # recursive handle child url\n",
    "        if new_url not in visited_links and new_url.startswith(url):\n",
    "            crawl(new_url, driver, output_file, depth + 1, max_depth, visited_links, content_hashes)\n",
    "\n",
    "\n",
    "def run_crawl(website_url, chrome_driver_path, output_file):\n",
    "    # check file if exist, rm\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "\n",
    "    driver = init_driver(chrome_driver_path)\n",
    "    try:\n",
    "        crawl(website_url, driver, output_file)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "#data config\n",
    "website_url = 'https://vtv.vn/'\n",
    "chrome_driver_path = r'D:\\Apps\\chromedriver-win64\\chromedriver.exe'\n",
    "output_file = 'vtv_data1809.txt'\n",
    "\n",
    "\n",
    "run_crawl(website_url, chrome_driver_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a239f6fc-33a6-4c85-a0b6-7824dc388c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
